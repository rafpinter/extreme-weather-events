{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression \n",
    "\n",
    "## Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logistic_regression import MultinomialLogisticRegression\n",
    "from preprocess import Preprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_data = \"../../data/train.csv\"\n",
    "raw_test_data = \"../../data/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lat', 'TS', 'Z1000', 'U850', 'PS', 'PSL', 'TMQ', 'Z200', 'lon', 'T500',\n",
      "       'Label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "preproc = Preprocess()\n",
    "raw_data = preproc.load_data(raw_train_data)\n",
    "\n",
    "cols = ['lat', 'TS', 'Z1000', 'U850', 'PS', 'PSL', 'TMQ', 'Z200', 'lon', 'T500',\n",
    "        \"Label\"] \n",
    "\n",
    "train_df, train_data = preproc.preprocess_data(raw_data, drop_cols=[\"SNo\", \"time\"])\n",
    "train_df = train_df[cols]\n",
    "\n",
    "np.random.shuffle(train_data)\n",
    "X_train, y_train, X_valid, y_valid = preproc.train_valid_split(\n",
    "    train_df.to_numpy(), test_size=0.33\n",
    ")\n",
    "\n",
    "print(train_df.columns)\n",
    "\n",
    "X_train = preproc.normalize_data(X_train)\n",
    "X_valid = preproc.normalize_data(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafaelapinter/Library/Mobile Documents/com~apple~CloudDocs/1.UdeM/github-projects/extreme-weather-events-ift6380/notebooks/multinomial_kernel.py:104: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return -np.sum(np.log(probs[range(len(y)), y.astype(int)])) / len(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, cross entropy loss: 0.9501298154704545\n",
      "Epoch 200, cross entropy loss: 0.8792929785252196\n",
      "Epoch 300, cross entropy loss: 0.8350867709219738\n",
      "Epoch 400, cross entropy loss: 0.8052076194391802\n",
      "Epoch 500, cross entropy loss: 0.7839014278919796\n",
      "Epoch 600, cross entropy loss: 0.7680894699929763\n",
      "Epoch 700, cross entropy loss: 0.7559842531420717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafaelapinter/miniconda3/envs/dc1-env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/rafaelapinter/miniconda3/envs/dc1-env/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9479755589260906\n",
      "Epoch 200, cross entropy loss: 0.8760881667776241\n",
      "Epoch 300, cross entropy loss: 0.8312034484303122\n",
      "Epoch 400, cross entropy loss: 0.8008235072649074\n",
      "Epoch 500, cross entropy loss: 0.7791147032747601\n",
      "Epoch 600, cross entropy loss: 0.7629629139395093\n",
      "Epoch 700, cross entropy loss: 0.7505629873230977\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9515855171138582\n",
      "Epoch 200, cross entropy loss: 0.880698984505383\n",
      "Epoch 300, cross entropy loss: 0.836229179188427\n",
      "Epoch 400, cross entropy loss: 0.8059981161343677\n",
      "Epoch 500, cross entropy loss: 0.7842945413454416\n",
      "Epoch 600, cross entropy loss: 0.7680607192638882\n",
      "Epoch 700, cross entropy loss: 0.7555222049832551\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9575951083146982\n",
      "Epoch 200, cross entropy loss: 0.8912449271587208\n",
      "Epoch 300, cross entropy loss: 0.850058799889287\n",
      "Epoch 400, cross entropy loss: 0.8224692841367389\n",
      "Epoch 500, cross entropy loss: 0.8030478628568684\n",
      "Epoch 600, cross entropy loss: 0.788867090790117\n",
      "Epoch 700, cross entropy loss: 0.778214554185141\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9433563745564041\n",
      "Epoch 200, cross entropy loss: 0.8692691522107449\n",
      "Epoch 300, cross entropy loss: 0.8231537328171679\n",
      "Epoch 400, cross entropy loss: 0.7920082431276857\n",
      "Epoch 500, cross entropy loss: 0.7697818203559031\n",
      "Epoch 600, cross entropy loss: 0.75325746757024\n",
      "Epoch 700, cross entropy loss: 0.7405773288431748\n",
      "Learning rate: 0.01\n",
      "Regularization: 0.01\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7321367608559549\n",
      "Epoch 200, cross entropy loss: 0.7025375952097122\n",
      "Epoch 300, cross entropy loss: 0.6901944412188235\n",
      "Epoch 400, cross entropy loss: 0.6823958055239414\n",
      "Epoch 500, cross entropy loss: 0.6768100367932977\n",
      "Epoch 600, cross entropy loss: 0.6724842026230518\n",
      "Epoch 700, cross entropy loss: 0.6689317377061312\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7260089881654082\n",
      "Epoch 200, cross entropy loss: 0.6951892216172414\n",
      "Epoch 300, cross entropy loss: 0.6824265901905221\n",
      "Epoch 400, cross entropy loss: 0.6745348813429628\n",
      "Epoch 500, cross entropy loss: 0.6689894936779724\n",
      "Epoch 600, cross entropy loss: 0.6647578444269641\n",
      "Epoch 700, cross entropy loss: 0.6613199143177528\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7303862120905937\n",
      "Epoch 200, cross entropy loss: 0.6976012209883833\n",
      "Epoch 300, cross entropy loss: 0.6838139666265736\n",
      "Epoch 400, cross entropy loss: 0.6754589622860498\n",
      "Epoch 500, cross entropy loss: 0.66968691603559\n",
      "Epoch 600, cross entropy loss: 0.6653427362642088\n",
      "Epoch 700, cross entropy loss: 0.6618643155510399\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7580208148693242\n",
      "Epoch 200, cross entropy loss: 0.7354463685805482\n",
      "Epoch 300, cross entropy loss: 0.7257968946388255\n",
      "Epoch 400, cross entropy loss: 0.7188843288885033\n",
      "Epoch 500, cross entropy loss: 0.7134925190072884\n",
      "Epoch 600, cross entropy loss: 0.7090817747278675\n",
      "Epoch 700, cross entropy loss: 0.7053191579088062\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7154849531812578\n",
      "Epoch 200, cross entropy loss: 0.6842363289812474\n",
      "Epoch 300, cross entropy loss: 0.6715567073474773\n",
      "Epoch 400, cross entropy loss: 0.6637733813272558\n",
      "Epoch 500, cross entropy loss: 0.6583026811477212\n",
      "Epoch 600, cross entropy loss: 0.6541231137252147\n",
      "Epoch 700, cross entropy loss: 0.6507259786648066\n",
      "Learning rate: 0.1\n",
      "Regularization: 0.01\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9512138210336621\n",
      "Epoch 200, cross entropy loss: 0.8819067121064232\n",
      "Epoch 300, cross entropy loss: 0.839256586677626\n",
      "Epoch 400, cross entropy loss: 0.8108623239859083\n",
      "Epoch 500, cross entropy loss: 0.7909412160699446\n",
      "Epoch 600, cross entropy loss: 0.7764115417928805\n",
      "Epoch 700, cross entropy loss: 0.7654904043816932\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9490664129019392\n",
      "Epoch 200, cross entropy loss: 0.8787115144147145\n",
      "Epoch 300, cross entropy loss: 0.8353804962553021\n",
      "Epoch 400, cross entropy loss: 0.8064805432639808\n",
      "Epoch 500, cross entropy loss: 0.7861517188545325\n",
      "Epoch 600, cross entropy loss: 0.7712782416532897\n",
      "Epoch 700, cross entropy loss: 0.7600601396713401\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9526543416457006\n",
      "Epoch 200, cross entropy loss: 0.8832887633245836\n",
      "Epoch 300, cross entropy loss: 0.8403696801885132\n",
      "Epoch 400, cross entropy loss: 0.811621323600065\n",
      "Epoch 500, cross entropy loss: 0.7913044629651543\n",
      "Epoch 600, cross entropy loss: 0.7763589104897257\n",
      "Epoch 700, cross entropy loss: 0.7650151608530146\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9586642994236659\n",
      "Epoch 200, cross entropy loss: 0.893821987402865\n",
      "Epoch 300, cross entropy loss: 0.85417852662066\n",
      "Epoch 400, cross entropy loss: 0.8280654462582235\n",
      "Epoch 500, cross entropy loss: 0.8100199298434096\n",
      "Epoch 600, cross entropy loss: 0.7971075946137478\n",
      "Epoch 700, cross entropy loss: 0.7876185482022177\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9444647546521986\n",
      "Epoch 200, cross entropy loss: 0.8719373595608173\n",
      "Epoch 300, cross entropy loss: 0.8274023037725691\n",
      "Epoch 400, cross entropy loss: 0.7977613538698415\n",
      "Epoch 500, cross entropy loss: 0.7769377950052109\n",
      "Epoch 600, cross entropy loss: 0.7617136718430217\n",
      "Epoch 700, cross entropy loss: 0.750236843557873\n",
      "Learning rate: 0.01\n",
      "Regularization: 0.05\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7447366167272998\n",
      "Epoch 200, cross entropy loss: 0.7213922654868579\n",
      "Epoch 300, cross entropy loss: 0.7127181010184017\n",
      "Epoch 400, cross entropy loss: 0.7079332540643144\n",
      "Epoch 500, cross entropy loss: 0.7052316545371079\n",
      "Epoch 600, cross entropy loss: 0.7037810974012515\n",
      "Epoch 700, cross entropy loss: 0.7031183269973738\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7386037209646752\n",
      "Epoch 200, cross entropy loss: 0.7141081410346156\n",
      "Epoch 300, cross entropy loss: 0.7050792738732198\n",
      "Epoch 400, cross entropy loss: 0.7002348524500486\n",
      "Epoch 500, cross entropy loss: 0.6975800467532991\n",
      "Epoch 600, cross entropy loss: 0.6962098294861475\n",
      "Epoch 700, cross entropy loss: 0.6956327647279423\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7430336656395536\n",
      "Epoch 200, cross entropy loss: 0.7169183200276528\n",
      "Epoch 300, cross entropy loss: 0.7073020683244478\n",
      "Epoch 400, cross entropy loss: 0.7024453750648161\n",
      "Epoch 500, cross entropy loss: 0.7000289957539325\n",
      "Epoch 600, cross entropy loss: 0.6990331539680944\n",
      "Epoch 700, cross entropy loss: 0.6989264365794955\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7704113980575836\n",
      "Epoch 200, cross entropy loss: 0.7534933206684795\n",
      "Epoch 300, cross entropy loss: 0.7469587115934864\n",
      "Epoch 400, cross entropy loss: 0.7426413274872662\n",
      "Epoch 500, cross entropy loss: 0.7397990960461965\n",
      "Epoch 600, cross entropy loss: 0.7379768226145843\n",
      "Epoch 700, cross entropy loss: 0.7368488966990165\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7283078621417063\n",
      "Epoch 200, cross entropy loss: 0.7035760344292958\n",
      "Epoch 300, cross entropy loss: 0.6947533871344546\n",
      "Epoch 400, cross entropy loss: 0.6901024668796927\n",
      "Epoch 500, cross entropy loss: 0.6876026497720057\n",
      "Epoch 600, cross entropy loss: 0.6863726959789553\n",
      "Epoch 700, cross entropy loss: 0.6859346867080053\n",
      "Learning rate: 0.1\n",
      "Regularization: 0.05\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9525688279876718\n",
      "Epoch 200, cross entropy loss: 0.8851738790829279\n",
      "Epoch 300, cross entropy loss: 0.8444688563721912\n",
      "Epoch 400, cross entropy loss: 0.8179307046693184\n",
      "Epoch 500, cross entropy loss: 0.7997409512924012\n",
      "Epoch 600, cross entropy loss: 0.7868141315427608\n",
      "Epoch 700, cross entropy loss: 0.7773730934312205\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9504299803717497\n",
      "Epoch 200, cross entropy loss: 0.8819906989610775\n",
      "Epoch 300, cross entropy loss: 0.8406018060365393\n",
      "Epoch 400, cross entropy loss: 0.8135518382628224\n",
      "Epoch 500, cross entropy loss: 0.7949479883292481\n",
      "Epoch 600, cross entropy loss: 0.7816724012955152\n",
      "Epoch 700, cross entropy loss: 0.7719315801066426\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9539903723105034\n",
      "Epoch 200, cross entropy loss: 0.8865259868485843\n",
      "Epoch 300, cross entropy loss: 0.8455453064386211\n",
      "Epoch 400, cross entropy loss: 0.8186503329321867\n",
      "Epoch 500, cross entropy loss: 0.8000668649897952\n",
      "Epoch 600, cross entropy loss: 0.7867316495220227\n",
      "Epoch 700, cross entropy loss: 0.7768813556902139\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9600007883098757\n",
      "Epoch 200, cross entropy loss: 0.8970433127080454\n",
      "Epoch 300, cross entropy loss: 0.8593281850348762\n",
      "Epoch 400, cross entropy loss: 0.8350606489100793\n",
      "Epoch 500, cross entropy loss: 0.8187350135765858\n",
      "Epoch 600, cross entropy loss: 0.8074082243932863\n",
      "Epoch 700, cross entropy loss: 0.7993735407235635\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9458502297719418\n",
      "Epoch 200, cross entropy loss: 0.8752726187484079\n",
      "Epoch 300, cross entropy loss: 0.8327130174668207\n",
      "Epoch 400, cross entropy loss: 0.8049527422975364\n",
      "Epoch 500, cross entropy loss: 0.7858827633168456\n",
      "Epoch 600, cross entropy loss: 0.7722839271839987\n",
      "Epoch 700, cross entropy loss: 0.7623112369512457\n",
      "Learning rate: 0.01\n",
      "Regularization: 0.1\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7604864365664811\n",
      "Epoch 200, cross entropy loss: 0.7449606033332898\n",
      "Epoch 300, cross entropy loss: 0.7408726757678747\n",
      "Epoch 400, cross entropy loss: 0.7398550647397804\n",
      "Epoch 500, cross entropy loss: 0.7407586767168708\n",
      "Epoch 600, cross entropy loss: 0.7429022158740011\n",
      "Epoch 700, cross entropy loss: 0.7458515636114263\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7543471369637591\n",
      "Epoch 200, cross entropy loss: 0.7377567903063335\n",
      "Epoch 300, cross entropy loss: 0.7333951284765916\n",
      "Epoch 400, cross entropy loss: 0.7323598163339057\n",
      "Epoch 500, cross entropy loss: 0.7333182380974574\n",
      "Epoch 600, cross entropy loss: 0.7355248108101267\n",
      "Epoch 700, cross entropy loss: 0.7385238277406789\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7588429825757533\n",
      "Epoch 200, cross entropy loss: 0.7410646938267397\n",
      "Epoch 300, cross entropy loss: 0.7366621954467905\n",
      "Epoch 400, cross entropy loss: 0.7361783910382739\n",
      "Epoch 500, cross entropy loss: 0.7379565954018601\n",
      "Epoch 600, cross entropy loss: 0.7411461760979513\n",
      "Epoch 700, cross entropy loss: 0.7452540878650649\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7858996270429074\n",
      "Epoch 200, cross entropy loss: 0.7760520107783937\n",
      "Epoch 300, cross entropy loss: 0.7734109827868125\n",
      "Epoch 400, cross entropy loss: 0.7723375757357199\n",
      "Epoch 500, cross entropy loss: 0.7726823173448317\n",
      "Epoch 600, cross entropy loss: 0.7740956324729797\n",
      "Epoch 700, cross entropy loss: 0.7762610701867791\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7443364983422668\n",
      "Epoch 200, cross entropy loss: 0.727750666239356\n",
      "Epoch 300, cross entropy loss: 0.7237492368681758\n",
      "Epoch 400, cross entropy loss: 0.7230138238202385\n",
      "Epoch 500, cross entropy loss: 0.7242276105523613\n",
      "Epoch 600, cross entropy loss: 0.7266846737961309\n",
      "Epoch 700, cross entropy loss: 0.7299455717620031\n",
      "Learning rate: 0.1\n",
      "Regularization: 0.1\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "regularizers = np.array([0.01, 0.05, 0.1])\n",
    "learning_rates = np.array([0.01, 0.1])\n",
    "\n",
    "params_combination = np.array(\n",
    "    np.meshgrid(regularizers, learning_rates)\n",
    ").T.reshape(-1, 2)\n",
    "\n",
    "cross_validation_accuracies = []\n",
    "cross_validation_accuracies_means = []\n",
    "train_errors = []\n",
    "validation_errors = []\n",
    "\n",
    "for reg, l_rate in params_combination:\n",
    "    model = MultinomialLogisticRegression(learning_rate=l_rate, num_iterations=800, regularizer=reg)\n",
    "    model.cross_validation(X_train, y_train)\n",
    "    print(\"Learning rate:\", l_rate)\n",
    "    print(\"Regularization:\", reg)\n",
    "    cross_validation_accuracies.append(model.cross_validation_accuracy)\n",
    "    cross_validation_accuracies_means.append(np.mean(model.cross_validation_accuracy))\n",
    "    train_errors.append(model.cross_valid_train_errors)\n",
    "    validation_errors.append(model.cross_valid_valid_errors)\n",
    "    \n",
    "    print(np.mean(model.cross_validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafaelapinter/Library/Mobile Documents/com~apple~CloudDocs/1.UdeM/github-projects/extreme-weather-events-ift6380/notebooks/multinomial_kernel.py:104: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return -np.sum(np.log(probs[range(len(y)), y.astype(int)])) / len(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, cross entropy loss: 1.0722998972186895\n",
      "Epoch 200, cross entropy loss: 1.0541240378000185\n",
      "Epoch 300, cross entropy loss: 1.0407770418451967\n",
      "Epoch 400, cross entropy loss: 1.0304141336304076\n",
      "Epoch 500, cross entropy loss: 1.0219747696440882\n",
      "Epoch 600, cross entropy loss: 1.0148280245748518\n",
      "Epoch 700, cross entropy loss: 1.00858509329827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafaelapinter/miniconda3/envs/dc1-env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/rafaelapinter/miniconda3/envs/dc1-env/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.071582533088183\n",
      "Epoch 200, cross entropy loss: 1.0529069928457107\n",
      "Epoch 300, cross entropy loss: 1.0391941512232514\n",
      "Epoch 400, cross entropy loss: 1.0285529757667775\n",
      "Epoch 500, cross entropy loss: 1.0198960955066707\n",
      "Epoch 600, cross entropy loss: 1.0125761420196902\n",
      "Epoch 700, cross entropy loss: 1.0061936948560344\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.0734647894607419\n",
      "Epoch 200, cross entropy loss: 1.0559753321709122\n",
      "Epoch 300, cross entropy loss: 1.04303079176171\n",
      "Epoch 400, cross entropy loss: 1.0328978758980856\n",
      "Epoch 500, cross entropy loss: 1.0245821086874118\n",
      "Epoch 600, cross entropy loss: 1.0174929730568323\n",
      "Epoch 700, cross entropy loss: 1.0112670194602436\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.0708106426442894\n",
      "Epoch 200, cross entropy loss: 1.0516202375651733\n",
      "Epoch 300, cross entropy loss: 1.0375521952544704\n",
      "Epoch 400, cross entropy loss: 1.0266551857035056\n",
      "Epoch 500, cross entropy loss: 1.017803570703586\n",
      "Epoch 600, cross entropy loss: 1.0103252852112603\n",
      "Epoch 700, cross entropy loss: 1.0038048367692822\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.07329237227938\n",
      "Epoch 200, cross entropy loss: 1.055914619266887\n",
      "Epoch 300, cross entropy loss: 1.0432339116086957\n",
      "Epoch 400, cross entropy loss: 1.0334424370592246\n",
      "Epoch 500, cross entropy loss: 1.025503071295951\n",
      "Epoch 600, cross entropy loss: 1.018800655028034\n",
      "Epoch 700, cross entropy loss: 1.0129574797051069\n",
      "Learning rate: 0.001\n",
      "Regularization: 0.01\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9930931203164313\n",
      "Epoch 200, cross entropy loss: 0.9564842578387677\n",
      "Epoch 300, cross entropy loss: 0.9281706391273328\n",
      "Epoch 400, cross entropy loss: 0.9041422671028037\n",
      "Epoch 500, cross entropy loss: 0.883491164020974\n",
      "Epoch 600, cross entropy loss: 0.8657926514839243\n",
      "Epoch 700, cross entropy loss: 0.8506984835653713\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9904168819889245\n",
      "Epoch 200, cross entropy loss: 0.9536545834415437\n",
      "Epoch 300, cross entropy loss: 0.9257113248703952\n",
      "Epoch 400, cross entropy loss: 0.9022402387616036\n",
      "Epoch 500, cross entropy loss: 0.8821787045159557\n",
      "Epoch 600, cross entropy loss: 0.8650237154462858\n",
      "Epoch 700, cross entropy loss: 0.8503905387771267\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.995720349916575\n",
      "Epoch 200, cross entropy loss: 0.958947236338334\n",
      "Epoch 300, cross entropy loss: 0.9309155604351678\n",
      "Epoch 400, cross entropy loss: 0.9074625813345297\n",
      "Epoch 500, cross entropy loss: 0.887542005160541\n",
      "Epoch 600, cross entropy loss: 0.8706464792503553\n",
      "Epoch 700, cross entropy loss: 0.8563795114055764\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9876503209334133\n",
      "Epoch 200, cross entropy loss: 0.9493364435308859\n",
      "Epoch 300, cross entropy loss: 0.919493400976636\n",
      "Epoch 400, cross entropy loss: 0.8941613361661656\n",
      "Epoch 500, cross entropy loss: 0.8724690289768215\n",
      "Epoch 600, cross entropy loss: 0.853978311053837\n",
      "Epoch 700, cross entropy loss: 0.8383058859856547\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9984769865685917\n",
      "Epoch 200, cross entropy loss: 0.9639848504916465\n",
      "Epoch 300, cross entropy loss: 0.9366853794494214\n",
      "Epoch 400, cross entropy loss: 0.9129762628835613\n",
      "Epoch 500, cross entropy loss: 0.8921902923759885\n",
      "Epoch 600, cross entropy loss: 0.8740692297319294\n",
      "Epoch 700, cross entropy loss: 0.8583811793859362\n",
      "Learning rate: 0.01\n",
      "Regularization: 0.01\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8172343506687483\n",
      "Epoch 200, cross entropy loss: 0.7732471225637176\n",
      "Epoch 300, cross entropy loss: 0.7586451548753912\n",
      "Epoch 400, cross entropy loss: 0.7503063256625161\n",
      "Epoch 500, cross entropy loss: 0.7442531819066079\n",
      "Epoch 600, cross entropy loss: 0.7394970732142594\n",
      "Epoch 700, cross entropy loss: 0.7356425884810921\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8177749434966368\n",
      "Epoch 200, cross entropy loss: 0.772942632168669\n",
      "Epoch 300, cross entropy loss: 0.7562838352255205\n",
      "Epoch 400, cross entropy loss: 0.7463238968375684\n",
      "Epoch 500, cross entropy loss: 0.7392415684840389\n",
      "Epoch 600, cross entropy loss: 0.7339328684066847\n",
      "Epoch 700, cross entropy loss: 0.7298642972505707\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8253355598180252\n",
      "Epoch 200, cross entropy loss: 0.787737420762893\n",
      "Epoch 300, cross entropy loss: 0.7777274383749921\n",
      "Epoch 400, cross entropy loss: 0.7728435854499156\n",
      "Epoch 500, cross entropy loss: 0.7693953626150107\n",
      "Epoch 600, cross entropy loss: 0.7665827826912819\n",
      "Epoch 700, cross entropy loss: 0.7641557937079195\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8039734578099045\n",
      "Epoch 200, cross entropy loss: 0.7607246548130879\n",
      "Epoch 300, cross entropy loss: 0.7477352081212563\n",
      "Epoch 400, cross entropy loss: 0.7408399093755241\n",
      "Epoch 500, cross entropy loss: 0.7359629992001906\n",
      "Epoch 600, cross entropy loss: 0.7321509042034231\n",
      "Epoch 700, cross entropy loss: 0.7290567956583496\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8227805964025042\n",
      "Epoch 200, cross entropy loss: 0.7730122604330678\n",
      "Epoch 300, cross entropy loss: 0.7549077024009179\n",
      "Epoch 400, cross entropy loss: 0.7441909857752941\n",
      "Epoch 500, cross entropy loss: 0.7364495885397133\n",
      "Epoch 600, cross entropy loss: 0.730481169030214\n",
      "Epoch 700, cross entropy loss: 0.7257519725514251\n",
      "Learning rate: 0.1\n",
      "Regularization: 0.01\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.0723347059638144\n",
      "Epoch 200, cross entropy loss: 1.0542395097673491\n",
      "Epoch 300, cross entropy loss: 1.0409971815105379\n",
      "Epoch 400, cross entropy loss: 1.0307519727612553\n",
      "Epoch 500, cross entropy loss: 1.0224378033797632\n",
      "Epoch 600, cross entropy loss: 1.015420925133059\n",
      "Epoch 700, cross entropy loss: 1.0093110604826552\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.0716182618724959\n",
      "Epoch 200, cross entropy loss: 1.0530254230492215\n",
      "Epoch 300, cross entropy loss: 1.0394197527359592\n",
      "Epoch 400, cross entropy loss: 1.0288989257837184\n",
      "Epoch 500, cross entropy loss: 1.0203698766730955\n",
      "Epoch 600, cross entropy loss: 1.0131823358684602\n",
      "Epoch 700, cross entropy loss: 1.0069353722097847\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.0734983747960911\n",
      "Epoch 200, cross entropy loss: 1.0560871370342944\n",
      "Epoch 300, cross entropy loss: 1.0432446492744694\n",
      "Epoch 400, cross entropy loss: 1.0332270877023189\n",
      "Epoch 500, cross entropy loss: 1.025034590781424\n",
      "Epoch 600, cross entropy loss: 1.018073833799945\n",
      "Epoch 700, cross entropy loss: 1.011979850052689\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.0708460714614316\n",
      "Epoch 200, cross entropy loss: 1.0517373206983984\n",
      "Epoch 300, cross entropy loss: 1.0377746114592992\n",
      "Epoch 400, cross entropy loss: 1.0269953991348002\n",
      "Epoch 500, cross entropy loss: 1.01826847551758\n",
      "Epoch 600, cross entropy loss: 1.0109190110466757\n",
      "Epoch 700, cross entropy loss: 1.0045301351420743\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.073326999182121\n",
      "Epoch 200, cross entropy loss: 1.0560296450563194\n",
      "Epoch 300, cross entropy loss: 1.04345348051585\n",
      "Epoch 400, cross entropy loss: 1.0337798000226774\n",
      "Epoch 500, cross entropy loss: 1.0259659604630693\n",
      "Epoch 600, cross entropy loss: 1.019393971440446\n",
      "Epoch 700, cross entropy loss: 1.0136846333989045\n",
      "Learning rate: 0.001\n",
      "Regularization: 0.05\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9942352713333616\n",
      "Epoch 200, cross entropy loss: 0.9590574463866668\n",
      "Epoch 300, cross entropy loss: 0.9321539672245965\n",
      "Epoch 400, cross entropy loss: 0.9094577794314816\n",
      "Epoch 500, cross entropy loss: 0.890052263546086\n",
      "Epoch 600, cross entropy loss: 0.8735161523060765\n",
      "Epoch 700, cross entropy loss: 0.8595069408302662\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9915812127629855\n",
      "Epoch 200, cross entropy loss: 0.9562619437882329\n",
      "Epoch 300, cross entropy loss: 0.9297316030438585\n",
      "Epoch 400, cross entropy loss: 0.9075917960916107\n",
      "Epoch 500, cross entropy loss: 0.888774106678319\n",
      "Epoch 600, cross entropy loss: 0.8727798984666366\n",
      "Epoch 700, cross entropy loss: 0.8592302836648481\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.996847667584245\n",
      "Epoch 200, cross entropy loss: 0.9615035608421625\n",
      "Epoch 300, cross entropy loss: 0.9348733782344042\n",
      "Epoch 400, cross entropy loss: 0.9127383584210021\n",
      "Epoch 500, cross entropy loss: 0.8940465160410322\n",
      "Epoch 600, cross entropy loss: 0.8782955620904894\n",
      "Epoch 700, cross entropy loss: 0.8650954408004786\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.988785600608393\n",
      "Epoch 200, cross entropy loss: 0.9518830545743627\n",
      "Epoch 300, cross entropy loss: 0.9234466992136959\n",
      "Epoch 400, cross entropy loss: 0.899457173583709\n",
      "Epoch 500, cross entropy loss: 0.8790292999790751\n",
      "Epoch 600, cross entropy loss: 0.8617247323969656\n",
      "Epoch 700, cross entropy loss: 0.8471637367051763\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9996236750802437\n",
      "Epoch 200, cross entropy loss: 0.9665796463634941\n",
      "Epoch 300, cross entropy loss: 0.9407081576207236\n",
      "Epoch 400, cross entropy loss: 0.9183450957860689\n",
      "Epoch 500, cross entropy loss: 0.89881365945899\n",
      "Epoch 600, cross entropy loss: 0.8818597836531412\n",
      "Epoch 700, cross entropy loss: 0.8672586428673166\n",
      "Learning rate: 0.01\n",
      "Regularization: 0.05\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8289493998248025\n",
      "Epoch 200, cross entropy loss: 0.7917088345854054\n",
      "Epoch 300, cross entropy loss: 0.7820714091908616\n",
      "Epoch 400, cross entropy loss: 0.7781482839478043\n",
      "Epoch 500, cross entropy loss: 0.7762943248650012\n",
      "Epoch 600, cross entropy loss: 0.7756219281448524\n",
      "Epoch 700, cross entropy loss: 0.7757785844465405\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8295164619194235\n",
      "Epoch 200, cross entropy loss: 0.7914090269077898\n",
      "Epoch 300, cross entropy loss: 0.7797126487299209\n",
      "Epoch 400, cross entropy loss: 0.7741881885748277\n",
      "Epoch 500, cross entropy loss: 0.7713351956599696\n",
      "Epoch 600, cross entropy loss: 0.7701414867320353\n",
      "Epoch 700, cross entropy loss: 0.7701105556132839\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8369026739348563\n",
      "Epoch 200, cross entropy loss: 0.8058485360888072\n",
      "Epoch 300, cross entropy loss: 0.8005820743061645\n",
      "Epoch 400, cross entropy loss: 0.7998900906938535\n",
      "Epoch 500, cross entropy loss: 0.8004159627195342\n",
      "Epoch 600, cross entropy loss: 0.8014623262034206\n",
      "Epoch 700, cross entropy loss: 0.8028253073574496\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8158241338129407\n",
      "Epoch 200, cross entropy loss: 0.7795553441296085\n",
      "Epoch 300, cross entropy loss: 0.7716655053817559\n",
      "Epoch 400, cross entropy loss: 0.7692686225450251\n",
      "Epoch 500, cross entropy loss: 0.7686484461428698\n",
      "Epoch 600, cross entropy loss: 0.7689625877537279\n",
      "Epoch 700, cross entropy loss: 0.7699109232760173\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8345652842183122\n",
      "Epoch 200, cross entropy loss: 0.7916302830389554\n",
      "Epoch 300, cross entropy loss: 0.7786935085116842\n",
      "Epoch 400, cross entropy loss: 0.7726431741261898\n",
      "Epoch 500, cross entropy loss: 0.7693742203805884\n",
      "Epoch 600, cross entropy loss: 0.7677771626134179\n",
      "Epoch 700, cross entropy loss: 0.7673573492780125\n",
      "Learning rate: 0.1\n",
      "Regularization: 0.05\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.0723782168952205\n",
      "Epoch 200, cross entropy loss: 1.0543838497265121\n",
      "Epoch 300, cross entropy loss: 1.0412723560922146\n",
      "Epoch 400, cross entropy loss: 1.031174271674815\n",
      "Epoch 500, cross entropy loss: 1.023016595549357\n",
      "Epoch 600, cross entropy loss: 1.0161620508308185\n",
      "Epoch 700, cross entropy loss: 1.0102185194631368\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.071662922852887\n",
      "Epoch 200, cross entropy loss: 1.0531734608036099\n",
      "Epoch 300, cross entropy loss: 1.0397017546268439\n",
      "Epoch 400, cross entropy loss: 1.0293313633048946\n",
      "Epoch 500, cross entropy loss: 1.0209621031311265\n",
      "Epoch 600, cross entropy loss: 1.0139400781794228\n",
      "Epoch 700, cross entropy loss: 1.0078624689019728\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.0735403564652775\n",
      "Epoch 200, cross entropy loss: 1.0562268931135217\n",
      "Epoch 300, cross entropy loss: 1.043511971165419\n",
      "Epoch 400, cross entropy loss: 1.0336386024576105\n",
      "Epoch 500, cross entropy loss: 1.0256001933989392\n",
      "Epoch 600, cross entropy loss: 1.0187999097288363\n",
      "Epoch 700, cross entropy loss: 1.0128708882932456\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.0708903574828592\n",
      "Epoch 200, cross entropy loss: 1.0518836746149298\n",
      "Epoch 300, cross entropy loss: 1.0380526317153351\n",
      "Epoch 400, cross entropy loss: 1.0274206659239185\n",
      "Epoch 500, cross entropy loss: 1.0188496065350723\n",
      "Epoch 600, cross entropy loss: 1.011661168340945\n",
      "Epoch 700, cross entropy loss: 1.0054367581080643\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.0733702828105474\n",
      "Epoch 200, cross entropy loss: 1.0561734272931096\n",
      "Epoch 300, cross entropy loss: 1.0437279416497929\n",
      "Epoch 400, cross entropy loss: 1.0342015037269934\n",
      "Epoch 500, cross entropy loss: 1.0265445719219677\n",
      "Epoch 600, cross entropy loss: 1.0201356169559612\n",
      "Epoch 700, cross entropy loss: 1.0145935755161513\n",
      "Learning rate: 0.001\n",
      "Regularization: 0.1\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9956629601045246\n",
      "Epoch 200, cross entropy loss: 0.9622739320715408\n",
      "Epoch 300, cross entropy loss: 0.9371331273461763\n",
      "Epoch 400, cross entropy loss: 0.9161021698423286\n",
      "Epoch 500, cross entropy loss: 0.8982536379524757\n",
      "Epoch 600, cross entropy loss: 0.8831705283337666\n",
      "Epoch 700, cross entropy loss: 0.870517512411385\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9930366262305618\n",
      "Epoch 200, cross entropy loss: 0.9595211442215946\n",
      "Epoch 300, cross entropy loss: 0.9347569507606878\n",
      "Epoch 400, cross entropy loss: 0.9142812427541194\n",
      "Epoch 500, cross entropy loss: 0.8970183593812732\n",
      "Epoch 600, cross entropy loss: 0.8824751272420753\n",
      "Epoch 700, cross entropy loss: 0.8702799647744999\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9982568146688325\n",
      "Epoch 200, cross entropy loss: 0.9646989664719482\n",
      "Epoch 300, cross entropy loss: 0.9398206504834499\n",
      "Epoch 400, cross entropy loss: 0.9193330797790925\n",
      "Epoch 500, cross entropy loss: 0.9021771546416463\n",
      "Epoch 600, cross entropy loss: 0.887856915640657\n",
      "Epoch 700, cross entropy loss: 0.8759903525441062\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9902047002021176\n",
      "Epoch 200, cross entropy loss: 0.9550663183787087\n",
      "Epoch 300, cross entropy loss: 0.9283883220100208\n",
      "Epoch 400, cross entropy loss: 0.9060769703556383\n",
      "Epoch 500, cross entropy loss: 0.887229638731892\n",
      "Epoch 600, cross entropy loss: 0.8714077590758762\n",
      "Epoch 700, cross entropy loss: 0.8582360501045782\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.0010570357198088\n",
      "Epoch 200, cross entropy loss: 0.9698231412033037\n",
      "Epoch 300, cross entropy loss: 0.9457366303348512\n",
      "Epoch 400, cross entropy loss: 0.9250561369142033\n",
      "Epoch 500, cross entropy loss: 0.907092868312742\n",
      "Epoch 600, cross entropy loss: 0.8915979760546562\n",
      "Epoch 700, cross entropy loss: 0.8783554722190416\n",
      "Learning rate: 0.01\n",
      "Regularization: 0.1\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8435932112698702\n",
      "Epoch 200, cross entropy loss: 0.8147859746125149\n",
      "Epoch 300, cross entropy loss: 0.8113542270851993\n",
      "Epoch 400, cross entropy loss: 0.8129507318044146\n",
      "Epoch 500, cross entropy loss: 0.8163457535629929\n",
      "Epoch 600, cross entropy loss: 0.8207779968080936\n",
      "Epoch 700, cross entropy loss: 0.8259485794033503\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.844193359947907\n",
      "Epoch 200, cross entropy loss: 0.814492020331691\n",
      "Epoch 300, cross entropy loss: 0.8089986656104212\n",
      "Epoch 400, cross entropy loss: 0.8090185532464018\n",
      "Epoch 500, cross entropy loss: 0.8114522296298827\n",
      "Epoch 600, cross entropy loss: 0.8154022596387228\n",
      "Epoch 700, cross entropy loss: 0.8204183785666745\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8513615665808953\n",
      "Epoch 200, cross entropy loss: 0.8284874302461999\n",
      "Epoch 300, cross entropy loss: 0.8291503692201299\n",
      "Epoch 400, cross entropy loss: 0.8336982222487759\n",
      "Epoch 500, cross entropy loss: 0.8391917128501885\n",
      "Epoch 600, cross entropy loss: 0.8450617555935941\n",
      "Epoch 700, cross entropy loss: 0.8511621994193621\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8306374788167359\n",
      "Epoch 200, cross entropy loss: 0.803093705775259\n",
      "Epoch 300, cross entropy loss: 0.8015783769573801\n",
      "Epoch 400, cross entropy loss: 0.8048045140069015\n",
      "Epoch 500, cross entropy loss: 0.8095052548212184\n",
      "Epoch 600, cross entropy loss: 0.8149771921916087\n",
      "Epoch 700, cross entropy loss: 0.8209785827981021\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8492961439880723\n",
      "Epoch 200, cross entropy loss: 0.8149028112963149\n",
      "Epoch 300, cross entropy loss: 0.8084257661501423\n",
      "Epoch 400, cross entropy loss: 0.8082084095648092\n",
      "Epoch 500, cross entropy loss: 0.8105300101816826\n",
      "Epoch 600, cross entropy loss: 0.8143971545924236\n",
      "Epoch 700, cross entropy loss: 0.8193640701862465\n",
      "Learning rate: 0.1\n",
      "Regularization: 0.1\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "regularizers = np.array([0.01, 0.05, 0.1])\n",
    "learning_rates = np.array([0.001, 0.01, 0.1])\n",
    "\n",
    "params_combination = np.array(\n",
    "    np.meshgrid(regularizers, learning_rates)\n",
    ").T.reshape(-1, 2)\n",
    "\n",
    "cross_validation_accuracies = []\n",
    "cross_validation_accuracies_means = []\n",
    "train_errors = []\n",
    "validation_errors = []\n",
    "\n",
    "for reg, l_rate in params_combination:\n",
    "    model = MultinomialLogisticRegression(learning_rate=l_rate, num_iterations=800, regularizer=reg)\n",
    "    model.cross_validation(X_train, y_train)\n",
    "    print(\"Learning rate:\", l_rate)\n",
    "    print(\"Regularization:\", reg)\n",
    "    cross_validation_accuracies.append(model.cross_validation_accuracy)\n",
    "    cross_validation_accuracies_means.append(np.mean(model.cross_validation_accuracy))\n",
    "    train_errors.append(model.cross_valid_train_errors)\n",
    "    validation_errors.append(model.cross_valid_valid_errors)\n",
    "    \n",
    "    print(np.mean(model.cross_validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8091299469907995"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.mean(np.array(cross_validation_accuracies)[:,1:], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.mean(np.array(cross_validation_accuracies)[:,1:], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.1 ])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_combination[np.argmax(np.mean(np.array(cross_validation_accuracies)[:,1:], axis=1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7321367608559549\n",
      "Epoch 200, cross entropy loss: 0.7025375952097122\n",
      "Epoch 300, cross entropy loss: 0.6901944412188235\n",
      "Epoch 400, cross entropy loss: 0.6823958055239414\n",
      "Epoch 500, cross entropy loss: 0.6768100367932977\n",
      "Epoch 600, cross entropy loss: 0.6724842026230518\n",
      "Epoch 700, cross entropy loss: 0.6689317377061312\n",
      "Epoch 800, cross entropy loss: 0.6659021679045767\n",
      "Epoch 900, cross entropy loss: 0.663262203066028\n",
      "Index(['lat', 'TS', 'Z1000', 'U850', 'PS', 'PSL', 'TMQ', 'Z200', 'lon', 'T500',\n",
      "       'Label'],\n",
      "      dtype='object')\n",
      "Confusion Matrix:\n",
      "[[10302   217  1110]\n",
      " [  341   230     7]\n",
      " [ 1123     9  1432]] \n",
      "\n",
      "Accuracy:\n",
      "0.81 \n",
      "\n",
      "Precision:\n",
      "0.6472 \n",
      "\n",
      "Recall:\n",
      "0.6141 \n",
      "\n",
      "F1 Score:\n",
      "0.6302\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialLogisticRegression(learning_rate=0.1, num_iterations=1000, regularizer=0.01)\n",
    "model.fit(X_train, y_train, collist=raw_data.columns, valid_x=X_valid, valid_y=y_valid)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.predict(X_valid)\n",
    "print(train_df.columns)\n",
    "\n",
    "model.get_metrics(y_valid, predictions, return_values=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15a5fe7f0>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(\n",
    "    model.iters,\n",
    "    model.train_errors\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    model.iters,\n",
    "    model.valid_errors,\n",
    "    color='r'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.771 - submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = Preprocess()\n",
    "raw_data = preproc.load_data(raw_test_data)\n",
    "\n",
    "test_df, test_data = preproc.preprocess_data(raw_data, drop_cols=[\"SNo\", \"time\"], is_test=True)\n",
    "\n",
    "test_data = test_df[['lat', 'TS', 'Z1000', 'U850', 'PS', 'PSL', 'TMQ', 'Z200', 'lon', 'T500']].to_numpy()\n",
    "\n",
    "test_data = preproc.normalize_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(X=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "submition = raw_data['SNo'].reset_index().copy()\n",
    "submition['Label'] = pd.Series(y_pred_test)\n",
    "submition.drop(\"index\", axis=1, inplace=True)\n",
    "submition.to_csv(f\"predictions_{datetime.now()}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dc1-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
