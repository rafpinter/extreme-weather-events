{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multinomial_kernel import MultinomialLogisticRegression\n",
    "from preprocess import Preprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_data = \"../data/train.csv\"\n",
    "raw_test_data = \"../data/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lat', 'TS', 'Z1000', 'U850', 'PS', 'PSL', 'TMQ', 'Z200', 'lon', 'T500',\n",
      "       'Label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "preproc = Preprocess()\n",
    "raw_data = preproc.load_data(raw_train_data)\n",
    "\n",
    "cols = ['lat', 'TS', 'Z1000', 'U850', 'PS', 'PSL', 'TMQ', 'Z200', 'lon', 'T500',\n",
    "        \"Label\"] \n",
    "\n",
    "train_df, train_data = preproc.preprocess_data(raw_data, drop_cols=[\"SNo\", \"time\"])\n",
    "train_df = train_df[cols]\n",
    "\n",
    "np.random.shuffle(train_data)\n",
    "X_train, y_train, X_valid, y_valid = preproc.train_valid_split(\n",
    "    train_df.to_numpy(), test_size=0.33 #, random_state=42\n",
    ")\n",
    "\n",
    "print(train_df.columns)\n",
    "\n",
    "X_train = preproc.normalize_data(X_train)\n",
    "X_valid = preproc.normalize_data(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.998900e+04</td>\n",
       "      <td>2.998900e+04</td>\n",
       "      <td>2.998900e+04</td>\n",
       "      <td>2.998900e+04</td>\n",
       "      <td>2.998900e+04</td>\n",
       "      <td>2.998900e+04</td>\n",
       "      <td>2.998900e+04</td>\n",
       "      <td>2.998900e+04</td>\n",
       "      <td>2.998900e+04</td>\n",
       "      <td>2.998900e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-2.119616e-15</td>\n",
       "      <td>-1.526265e-13</td>\n",
       "      <td>-3.825070e-15</td>\n",
       "      <td>-7.439742e-17</td>\n",
       "      <td>-7.886519e-12</td>\n",
       "      <td>-8.770782e-12</td>\n",
       "      <td>-4.153046e-15</td>\n",
       "      <td>-1.322014e-12</td>\n",
       "      <td>-4.269559e-16</td>\n",
       "      <td>-8.289981e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "      <td>1.000017e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.230698e+00</td>\n",
       "      <td>-2.390289e+00</td>\n",
       "      <td>-1.702147e+00</td>\n",
       "      <td>-6.609335e+00</td>\n",
       "      <td>-9.061124e+00</td>\n",
       "      <td>-9.081592e+00</td>\n",
       "      <td>-2.163515e+00</td>\n",
       "      <td>-4.418123e+00</td>\n",
       "      <td>-9.816620e-01</td>\n",
       "      <td>-4.385711e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-9.439705e-01</td>\n",
       "      <td>-6.300049e-01</td>\n",
       "      <td>-7.746334e-01</td>\n",
       "      <td>-7.189999e-01</td>\n",
       "      <td>-7.180413e-01</td>\n",
       "      <td>-7.162969e-01</td>\n",
       "      <td>-8.368656e-01</td>\n",
       "      <td>-4.697931e-01</td>\n",
       "      <td>-6.698200e-01</td>\n",
       "      <td>-4.127475e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.842328e-01</td>\n",
       "      <td>1.121266e-01</td>\n",
       "      <td>-3.899279e-02</td>\n",
       "      <td>-1.584250e-01</td>\n",
       "      <td>-4.825500e-02</td>\n",
       "      <td>-4.485125e-02</td>\n",
       "      <td>1.238674e-02</td>\n",
       "      <td>2.263594e-01</td>\n",
       "      <td>-3.960074e-01</td>\n",
       "      <td>2.355560e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.083603e+00</td>\n",
       "      <td>9.078224e-01</td>\n",
       "      <td>6.105163e-01</td>\n",
       "      <td>6.058339e-01</td>\n",
       "      <td>5.840900e-01</td>\n",
       "      <td>5.831005e-01</td>\n",
       "      <td>7.572449e-01</td>\n",
       "      <td>7.615106e-01</td>\n",
       "      <td>1.896472e-01</td>\n",
       "      <td>7.117016e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.175766e+00</td>\n",
       "      <td>1.465700e+00</td>\n",
       "      <td>4.551860e+00</td>\n",
       "      <td>4.782046e+00</td>\n",
       "      <td>4.484265e+00</td>\n",
       "      <td>4.488067e+00</td>\n",
       "      <td>3.598686e+00</td>\n",
       "      <td>1.588394e+00</td>\n",
       "      <td>2.068306e+00</td>\n",
       "      <td>2.662933e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1             2             3             4  \\\n",
       "count  2.998900e+04  2.998900e+04  2.998900e+04  2.998900e+04  2.998900e+04   \n",
       "mean  -2.119616e-15 -1.526265e-13 -3.825070e-15 -7.439742e-17 -7.886519e-12   \n",
       "std    1.000017e+00  1.000017e+00  1.000017e+00  1.000017e+00  1.000017e+00   \n",
       "min   -1.230698e+00 -2.390289e+00 -1.702147e+00 -6.609335e+00 -9.061124e+00   \n",
       "25%   -9.439705e-01 -6.300049e-01 -7.746334e-01 -7.189999e-01 -7.180413e-01   \n",
       "50%    6.842328e-01  1.121266e-01 -3.899279e-02 -1.584250e-01 -4.825500e-02   \n",
       "75%    1.083603e+00  9.078224e-01  6.105163e-01  6.058339e-01  5.840900e-01   \n",
       "max    1.175766e+00  1.465700e+00  4.551860e+00  4.782046e+00  4.484265e+00   \n",
       "\n",
       "                  5             6             7             8             9  \n",
       "count  2.998900e+04  2.998900e+04  2.998900e+04  2.998900e+04  2.998900e+04  \n",
       "mean  -8.770782e-12 -4.153046e-15 -1.322014e-12 -4.269559e-16 -8.289981e-13  \n",
       "std    1.000017e+00  1.000017e+00  1.000017e+00  1.000017e+00  1.000017e+00  \n",
       "min   -9.081592e+00 -2.163515e+00 -4.418123e+00 -9.816620e-01 -4.385711e+00  \n",
       "25%   -7.162969e-01 -8.368656e-01 -4.697931e-01 -6.698200e-01 -4.127475e-01  \n",
       "50%   -4.485125e-02  1.238674e-02  2.263594e-01 -3.960074e-01  2.355560e-01  \n",
       "75%    5.831005e-01  7.572449e-01  7.615106e-01  1.896472e-01  7.117016e-01  \n",
       "max    4.488067e+00  3.598686e+00  1.588394e+00  2.068306e+00  2.662933e+00  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29989,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:,0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafaelapinter/Library/Mobile Documents/com~apple~CloudDocs/1.UdeM/github-projects/extreme-weather-events-ift6380/notebooks/multinomial_kernel.py:104: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return -np.sum(np.log(probs[range(len(y)), y.astype(int)])) / len(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, cross entropy loss: 0.9501298154704545\n",
      "Epoch 200, cross entropy loss: 0.8792929785252196\n",
      "Epoch 300, cross entropy loss: 0.8350867709219738\n",
      "Epoch 400, cross entropy loss: 0.8052076194391802\n",
      "Epoch 500, cross entropy loss: 0.7839014278919796\n",
      "Epoch 600, cross entropy loss: 0.7680894699929763\n",
      "Epoch 700, cross entropy loss: 0.7559842531420717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafaelapinter/miniconda3/envs/dc1-env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/rafaelapinter/miniconda3/envs/dc1-env/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9479755589260906\n",
      "Epoch 200, cross entropy loss: 0.8760881667776241\n",
      "Epoch 300, cross entropy loss: 0.8312034484303122\n",
      "Epoch 400, cross entropy loss: 0.8008235072649074\n",
      "Epoch 500, cross entropy loss: 0.7791147032747601\n",
      "Epoch 600, cross entropy loss: 0.7629629139395093\n",
      "Epoch 700, cross entropy loss: 0.7505629873230977\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9515855171138582\n",
      "Epoch 200, cross entropy loss: 0.880698984505383\n",
      "Epoch 300, cross entropy loss: 0.836229179188427\n",
      "Epoch 400, cross entropy loss: 0.8059981161343677\n",
      "Epoch 500, cross entropy loss: 0.7842945413454416\n",
      "Epoch 600, cross entropy loss: 0.7680607192638882\n",
      "Epoch 700, cross entropy loss: 0.7555222049832551\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9575951083146982\n",
      "Epoch 200, cross entropy loss: 0.8912449271587208\n",
      "Epoch 300, cross entropy loss: 0.850058799889287\n",
      "Epoch 400, cross entropy loss: 0.8224692841367389\n",
      "Epoch 500, cross entropy loss: 0.8030478628568684\n",
      "Epoch 600, cross entropy loss: 0.788867090790117\n",
      "Epoch 700, cross entropy loss: 0.778214554185141\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9433563745564041\n",
      "Epoch 200, cross entropy loss: 0.8692691522107449\n",
      "Epoch 300, cross entropy loss: 0.8231537328171679\n",
      "Epoch 400, cross entropy loss: 0.7920082431276857\n",
      "Epoch 500, cross entropy loss: 0.7697818203559031\n",
      "Epoch 600, cross entropy loss: 0.75325746757024\n",
      "Epoch 700, cross entropy loss: 0.7405773288431748\n",
      "Learning rate: 0.01\n",
      "Regularization: 0.01\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7321367608559549\n",
      "Epoch 200, cross entropy loss: 0.7025375952097122\n",
      "Epoch 300, cross entropy loss: 0.6901944412188235\n",
      "Epoch 400, cross entropy loss: 0.6823958055239414\n",
      "Epoch 500, cross entropy loss: 0.6768100367932977\n",
      "Epoch 600, cross entropy loss: 0.6724842026230518\n",
      "Epoch 700, cross entropy loss: 0.6689317377061312\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7260089881654082\n",
      "Epoch 200, cross entropy loss: 0.6951892216172414\n",
      "Epoch 300, cross entropy loss: 0.6824265901905221\n",
      "Epoch 400, cross entropy loss: 0.6745348813429628\n",
      "Epoch 500, cross entropy loss: 0.6689894936779724\n",
      "Epoch 600, cross entropy loss: 0.6647578444269641\n",
      "Epoch 700, cross entropy loss: 0.6613199143177528\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7303862120905937\n",
      "Epoch 200, cross entropy loss: 0.6976012209883833\n",
      "Epoch 300, cross entropy loss: 0.6838139666265736\n",
      "Epoch 400, cross entropy loss: 0.6754589622860498\n",
      "Epoch 500, cross entropy loss: 0.66968691603559\n",
      "Epoch 600, cross entropy loss: 0.6653427362642088\n",
      "Epoch 700, cross entropy loss: 0.6618643155510399\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7580208148693242\n",
      "Epoch 200, cross entropy loss: 0.7354463685805482\n",
      "Epoch 300, cross entropy loss: 0.7257968946388255\n",
      "Epoch 400, cross entropy loss: 0.7188843288885033\n",
      "Epoch 500, cross entropy loss: 0.7134925190072884\n",
      "Epoch 600, cross entropy loss: 0.7090817747278675\n",
      "Epoch 700, cross entropy loss: 0.7053191579088062\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7154849531812578\n",
      "Epoch 200, cross entropy loss: 0.6842363289812474\n",
      "Epoch 300, cross entropy loss: 0.6715567073474773\n",
      "Epoch 400, cross entropy loss: 0.6637733813272558\n",
      "Epoch 500, cross entropy loss: 0.6583026811477212\n",
      "Epoch 600, cross entropy loss: 0.6541231137252147\n",
      "Epoch 700, cross entropy loss: 0.6507259786648066\n",
      "Learning rate: 0.1\n",
      "Regularization: 0.01\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9512138210336621\n",
      "Epoch 200, cross entropy loss: 0.8819067121064232\n",
      "Epoch 300, cross entropy loss: 0.839256586677626\n",
      "Epoch 400, cross entropy loss: 0.8108623239859083\n",
      "Epoch 500, cross entropy loss: 0.7909412160699446\n",
      "Epoch 600, cross entropy loss: 0.7764115417928805\n",
      "Epoch 700, cross entropy loss: 0.7654904043816932\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9490664129019392\n",
      "Epoch 200, cross entropy loss: 0.8787115144147145\n",
      "Epoch 300, cross entropy loss: 0.8353804962553021\n",
      "Epoch 400, cross entropy loss: 0.8064805432639808\n",
      "Epoch 500, cross entropy loss: 0.7861517188545325\n",
      "Epoch 600, cross entropy loss: 0.7712782416532897\n",
      "Epoch 700, cross entropy loss: 0.7600601396713401\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9526543416457006\n",
      "Epoch 200, cross entropy loss: 0.8832887633245836\n",
      "Epoch 300, cross entropy loss: 0.8403696801885132\n",
      "Epoch 400, cross entropy loss: 0.811621323600065\n",
      "Epoch 500, cross entropy loss: 0.7913044629651543\n",
      "Epoch 600, cross entropy loss: 0.7763589104897257\n",
      "Epoch 700, cross entropy loss: 0.7650151608530146\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9586642994236659\n",
      "Epoch 200, cross entropy loss: 0.893821987402865\n",
      "Epoch 300, cross entropy loss: 0.85417852662066\n",
      "Epoch 400, cross entropy loss: 0.8280654462582235\n",
      "Epoch 500, cross entropy loss: 0.8100199298434096\n",
      "Epoch 600, cross entropy loss: 0.7971075946137478\n",
      "Epoch 700, cross entropy loss: 0.7876185482022177\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9444647546521986\n",
      "Epoch 200, cross entropy loss: 0.8719373595608173\n",
      "Epoch 300, cross entropy loss: 0.8274023037725691\n",
      "Epoch 400, cross entropy loss: 0.7977613538698415\n",
      "Epoch 500, cross entropy loss: 0.7769377950052109\n",
      "Epoch 600, cross entropy loss: 0.7617136718430217\n",
      "Epoch 700, cross entropy loss: 0.750236843557873\n",
      "Learning rate: 0.01\n",
      "Regularization: 0.05\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7447366167272998\n",
      "Epoch 200, cross entropy loss: 0.7213922654868579\n",
      "Epoch 300, cross entropy loss: 0.7127181010184017\n",
      "Epoch 400, cross entropy loss: 0.7079332540643144\n",
      "Epoch 500, cross entropy loss: 0.7052316545371079\n",
      "Epoch 600, cross entropy loss: 0.7037810974012515\n",
      "Epoch 700, cross entropy loss: 0.7031183269973738\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7386037209646752\n",
      "Epoch 200, cross entropy loss: 0.7141081410346156\n",
      "Epoch 300, cross entropy loss: 0.7050792738732198\n",
      "Epoch 400, cross entropy loss: 0.7002348524500486\n",
      "Epoch 500, cross entropy loss: 0.6975800467532991\n",
      "Epoch 600, cross entropy loss: 0.6962098294861475\n",
      "Epoch 700, cross entropy loss: 0.6956327647279423\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7430336656395536\n",
      "Epoch 200, cross entropy loss: 0.7169183200276528\n",
      "Epoch 300, cross entropy loss: 0.7073020683244478\n",
      "Epoch 400, cross entropy loss: 0.7024453750648161\n",
      "Epoch 500, cross entropy loss: 0.7000289957539325\n",
      "Epoch 600, cross entropy loss: 0.6990331539680944\n",
      "Epoch 700, cross entropy loss: 0.6989264365794955\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7704113980575836\n",
      "Epoch 200, cross entropy loss: 0.7534933206684795\n",
      "Epoch 300, cross entropy loss: 0.7469587115934864\n",
      "Epoch 400, cross entropy loss: 0.7426413274872662\n",
      "Epoch 500, cross entropy loss: 0.7397990960461965\n",
      "Epoch 600, cross entropy loss: 0.7379768226145843\n",
      "Epoch 700, cross entropy loss: 0.7368488966990165\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7283078621417063\n",
      "Epoch 200, cross entropy loss: 0.7035760344292958\n",
      "Epoch 300, cross entropy loss: 0.6947533871344546\n",
      "Epoch 400, cross entropy loss: 0.6901024668796927\n",
      "Epoch 500, cross entropy loss: 0.6876026497720057\n",
      "Epoch 600, cross entropy loss: 0.6863726959789553\n",
      "Epoch 700, cross entropy loss: 0.6859346867080053\n",
      "Learning rate: 0.1\n",
      "Regularization: 0.05\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9525688279876718\n",
      "Epoch 200, cross entropy loss: 0.8851738790829279\n",
      "Epoch 300, cross entropy loss: 0.8444688563721912\n",
      "Epoch 400, cross entropy loss: 0.8179307046693184\n",
      "Epoch 500, cross entropy loss: 0.7997409512924012\n",
      "Epoch 600, cross entropy loss: 0.7868141315427608\n",
      "Epoch 700, cross entropy loss: 0.7773730934312205\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9504299803717497\n",
      "Epoch 200, cross entropy loss: 0.8819906989610775\n",
      "Epoch 300, cross entropy loss: 0.8406018060365393\n",
      "Epoch 400, cross entropy loss: 0.8135518382628224\n",
      "Epoch 500, cross entropy loss: 0.7949479883292481\n",
      "Epoch 600, cross entropy loss: 0.7816724012955152\n",
      "Epoch 700, cross entropy loss: 0.7719315801066426\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9539903723105034\n",
      "Epoch 200, cross entropy loss: 0.8865259868485843\n",
      "Epoch 300, cross entropy loss: 0.8455453064386211\n",
      "Epoch 400, cross entropy loss: 0.8186503329321867\n",
      "Epoch 500, cross entropy loss: 0.8000668649897952\n",
      "Epoch 600, cross entropy loss: 0.7867316495220227\n",
      "Epoch 700, cross entropy loss: 0.7768813556902139\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9600007883098757\n",
      "Epoch 200, cross entropy loss: 0.8970433127080454\n",
      "Epoch 300, cross entropy loss: 0.8593281850348762\n",
      "Epoch 400, cross entropy loss: 0.8350606489100793\n",
      "Epoch 500, cross entropy loss: 0.8187350135765858\n",
      "Epoch 600, cross entropy loss: 0.8074082243932863\n",
      "Epoch 700, cross entropy loss: 0.7993735407235635\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9458502297719418\n",
      "Epoch 200, cross entropy loss: 0.8752726187484079\n",
      "Epoch 300, cross entropy loss: 0.8327130174668207\n",
      "Epoch 400, cross entropy loss: 0.8049527422975364\n",
      "Epoch 500, cross entropy loss: 0.7858827633168456\n",
      "Epoch 600, cross entropy loss: 0.7722839271839987\n",
      "Epoch 700, cross entropy loss: 0.7623112369512457\n",
      "Learning rate: 0.01\n",
      "Regularization: 0.1\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7604864365664811\n",
      "Epoch 200, cross entropy loss: 0.7449606033332898\n",
      "Epoch 300, cross entropy loss: 0.7408726757678747\n",
      "Epoch 400, cross entropy loss: 0.7398550647397804\n",
      "Epoch 500, cross entropy loss: 0.7407586767168708\n",
      "Epoch 600, cross entropy loss: 0.7429022158740011\n",
      "Epoch 700, cross entropy loss: 0.7458515636114263\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7543471369637591\n",
      "Epoch 200, cross entropy loss: 0.7377567903063335\n",
      "Epoch 300, cross entropy loss: 0.7333951284765916\n",
      "Epoch 400, cross entropy loss: 0.7323598163339057\n",
      "Epoch 500, cross entropy loss: 0.7333182380974574\n",
      "Epoch 600, cross entropy loss: 0.7355248108101267\n",
      "Epoch 700, cross entropy loss: 0.7385238277406789\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7588429825757533\n",
      "Epoch 200, cross entropy loss: 0.7410646938267397\n",
      "Epoch 300, cross entropy loss: 0.7366621954467905\n",
      "Epoch 400, cross entropy loss: 0.7361783910382739\n",
      "Epoch 500, cross entropy loss: 0.7379565954018601\n",
      "Epoch 600, cross entropy loss: 0.7411461760979513\n",
      "Epoch 700, cross entropy loss: 0.7452540878650649\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7858996270429074\n",
      "Epoch 200, cross entropy loss: 0.7760520107783937\n",
      "Epoch 300, cross entropy loss: 0.7734109827868125\n",
      "Epoch 400, cross entropy loss: 0.7723375757357199\n",
      "Epoch 500, cross entropy loss: 0.7726823173448317\n",
      "Epoch 600, cross entropy loss: 0.7740956324729797\n",
      "Epoch 700, cross entropy loss: 0.7762610701867791\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.7443364983422668\n",
      "Epoch 200, cross entropy loss: 0.727750666239356\n",
      "Epoch 300, cross entropy loss: 0.7237492368681758\n",
      "Epoch 400, cross entropy loss: 0.7230138238202385\n",
      "Epoch 500, cross entropy loss: 0.7242276105523613\n",
      "Epoch 600, cross entropy loss: 0.7266846737961309\n",
      "Epoch 700, cross entropy loss: 0.7299455717620031\n",
      "Learning rate: 0.1\n",
      "Regularization: 0.1\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "regularizers = np.array([0.01, 0.05, 0.1])\n",
    "learning_rates = np.array([0.01, 0.1])\n",
    "\n",
    "params_combination = np.array(\n",
    "    np.meshgrid(regularizers, learning_rates)\n",
    ").T.reshape(-1, 2)\n",
    "\n",
    "cross_validation_accuracies = []\n",
    "cross_validation_accuracies_means = []\n",
    "train_errors = []\n",
    "validation_errors = []\n",
    "\n",
    "for reg, l_rate in params_combination:\n",
    "    model = MultinomialLogisticRegression(learning_rate=l_rate, num_iterations=800, regularizer=reg)\n",
    "    model.cross_validation(X_train, y_train)\n",
    "    print(\"Learning rate:\", l_rate)\n",
    "    print(\"Regularization:\", reg)\n",
    "    cross_validation_accuracies.append(model.cross_validation_accuracy)\n",
    "    cross_validation_accuracies_means.append(np.mean(model.cross_validation_accuracy))\n",
    "    train_errors.append(model.cross_valid_train_errors)\n",
    "    validation_errors.append(model.cross_valid_valid_errors)\n",
    "    \n",
    "    print(np.mean(model.cross_validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafaelapinter/Library/Mobile Documents/com~apple~CloudDocs/1.UdeM/github-projects/extreme-weather-events-ift6380/notebooks/multinomial_kernel.py:104: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return -np.sum(np.log(probs[range(len(y)), y.astype(int)])) / len(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, cross entropy loss: 1.0722998972186895\n",
      "Epoch 200, cross entropy loss: 1.0541240378000185\n",
      "Epoch 300, cross entropy loss: 1.0407770418451967\n",
      "Epoch 400, cross entropy loss: 1.0304141336304076\n",
      "Epoch 500, cross entropy loss: 1.0219747696440882\n",
      "Epoch 600, cross entropy loss: 1.0148280245748518\n",
      "Epoch 700, cross entropy loss: 1.00858509329827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafaelapinter/miniconda3/envs/dc1-env/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/rafaelapinter/miniconda3/envs/dc1-env/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.071582533088183\n",
      "Epoch 200, cross entropy loss: 1.0529069928457107\n",
      "Epoch 300, cross entropy loss: 1.0391941512232514\n",
      "Epoch 400, cross entropy loss: 1.0285529757667775\n",
      "Epoch 500, cross entropy loss: 1.0198960955066707\n",
      "Epoch 600, cross entropy loss: 1.0125761420196902\n",
      "Epoch 700, cross entropy loss: 1.0061936948560344\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.0734647894607419\n",
      "Epoch 200, cross entropy loss: 1.0559753321709122\n",
      "Epoch 300, cross entropy loss: 1.04303079176171\n",
      "Epoch 400, cross entropy loss: 1.0328978758980856\n",
      "Epoch 500, cross entropy loss: 1.0245821086874118\n",
      "Epoch 600, cross entropy loss: 1.0174929730568323\n",
      "Epoch 700, cross entropy loss: 1.0112670194602436\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.0708106426442894\n",
      "Epoch 200, cross entropy loss: 1.0516202375651733\n",
      "Epoch 300, cross entropy loss: 1.0375521952544704\n",
      "Epoch 400, cross entropy loss: 1.0266551857035056\n",
      "Epoch 500, cross entropy loss: 1.017803570703586\n",
      "Epoch 600, cross entropy loss: 1.0103252852112603\n",
      "Epoch 700, cross entropy loss: 1.0038048367692822\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.07329237227938\n",
      "Epoch 200, cross entropy loss: 1.055914619266887\n",
      "Epoch 300, cross entropy loss: 1.0432339116086957\n",
      "Epoch 400, cross entropy loss: 1.0334424370592246\n",
      "Epoch 500, cross entropy loss: 1.025503071295951\n",
      "Epoch 600, cross entropy loss: 1.018800655028034\n",
      "Epoch 700, cross entropy loss: 1.0129574797051069\n",
      "Learning rate: 0.001\n",
      "Regularization: 0.01\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9930931203164313\n",
      "Epoch 200, cross entropy loss: 0.9564842578387677\n",
      "Epoch 300, cross entropy loss: 0.9281706391273328\n",
      "Epoch 400, cross entropy loss: 0.9041422671028037\n",
      "Epoch 500, cross entropy loss: 0.883491164020974\n",
      "Epoch 600, cross entropy loss: 0.8657926514839243\n",
      "Epoch 700, cross entropy loss: 0.8506984835653713\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9904168819889245\n",
      "Epoch 200, cross entropy loss: 0.9536545834415437\n",
      "Epoch 300, cross entropy loss: 0.9257113248703952\n",
      "Epoch 400, cross entropy loss: 0.9022402387616036\n",
      "Epoch 500, cross entropy loss: 0.8821787045159557\n",
      "Epoch 600, cross entropy loss: 0.8650237154462858\n",
      "Epoch 700, cross entropy loss: 0.8503905387771267\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.995720349916575\n",
      "Epoch 200, cross entropy loss: 0.958947236338334\n",
      "Epoch 300, cross entropy loss: 0.9309155604351678\n",
      "Epoch 400, cross entropy loss: 0.9074625813345297\n",
      "Epoch 500, cross entropy loss: 0.887542005160541\n",
      "Epoch 600, cross entropy loss: 0.8706464792503553\n",
      "Epoch 700, cross entropy loss: 0.8563795114055764\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9876503209334133\n",
      "Epoch 200, cross entropy loss: 0.9493364435308859\n",
      "Epoch 300, cross entropy loss: 0.919493400976636\n",
      "Epoch 400, cross entropy loss: 0.8941613361661656\n",
      "Epoch 500, cross entropy loss: 0.8724690289768215\n",
      "Epoch 600, cross entropy loss: 0.853978311053837\n",
      "Epoch 700, cross entropy loss: 0.8383058859856547\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9984769865685917\n",
      "Epoch 200, cross entropy loss: 0.9639848504916465\n",
      "Epoch 300, cross entropy loss: 0.9366853794494214\n",
      "Epoch 400, cross entropy loss: 0.9129762628835613\n",
      "Epoch 500, cross entropy loss: 0.8921902923759885\n",
      "Epoch 600, cross entropy loss: 0.8740692297319294\n",
      "Epoch 700, cross entropy loss: 0.8583811793859362\n",
      "Learning rate: 0.01\n",
      "Regularization: 0.01\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8172343506687483\n",
      "Epoch 200, cross entropy loss: 0.7732471225637176\n",
      "Epoch 300, cross entropy loss: 0.7586451548753912\n",
      "Epoch 400, cross entropy loss: 0.7503063256625161\n",
      "Epoch 500, cross entropy loss: 0.7442531819066079\n",
      "Epoch 600, cross entropy loss: 0.7394970732142594\n",
      "Epoch 700, cross entropy loss: 0.7356425884810921\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8177749434966368\n",
      "Epoch 200, cross entropy loss: 0.772942632168669\n",
      "Epoch 300, cross entropy loss: 0.7562838352255205\n",
      "Epoch 400, cross entropy loss: 0.7463238968375684\n",
      "Epoch 500, cross entropy loss: 0.7392415684840389\n",
      "Epoch 600, cross entropy loss: 0.7339328684066847\n",
      "Epoch 700, cross entropy loss: 0.7298642972505707\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8253355598180252\n",
      "Epoch 200, cross entropy loss: 0.787737420762893\n",
      "Epoch 300, cross entropy loss: 0.7777274383749921\n",
      "Epoch 400, cross entropy loss: 0.7728435854499156\n",
      "Epoch 500, cross entropy loss: 0.7693953626150107\n",
      "Epoch 600, cross entropy loss: 0.7665827826912819\n",
      "Epoch 700, cross entropy loss: 0.7641557937079195\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8039734578099045\n",
      "Epoch 200, cross entropy loss: 0.7607246548130879\n",
      "Epoch 300, cross entropy loss: 0.7477352081212563\n",
      "Epoch 400, cross entropy loss: 0.7408399093755241\n",
      "Epoch 500, cross entropy loss: 0.7359629992001906\n",
      "Epoch 600, cross entropy loss: 0.7321509042034231\n",
      "Epoch 700, cross entropy loss: 0.7290567956583496\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8227805964025042\n",
      "Epoch 200, cross entropy loss: 0.7730122604330678\n",
      "Epoch 300, cross entropy loss: 0.7549077024009179\n",
      "Epoch 400, cross entropy loss: 0.7441909857752941\n",
      "Epoch 500, cross entropy loss: 0.7364495885397133\n",
      "Epoch 600, cross entropy loss: 0.730481169030214\n",
      "Epoch 700, cross entropy loss: 0.7257519725514251\n",
      "Learning rate: 0.1\n",
      "Regularization: 0.01\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.0723347059638144\n",
      "Epoch 200, cross entropy loss: 1.0542395097673491\n",
      "Epoch 300, cross entropy loss: 1.0409971815105379\n",
      "Epoch 400, cross entropy loss: 1.0307519727612553\n",
      "Epoch 500, cross entropy loss: 1.0224378033797632\n",
      "Epoch 600, cross entropy loss: 1.015420925133059\n",
      "Epoch 700, cross entropy loss: 1.0093110604826552\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.0716182618724959\n",
      "Epoch 200, cross entropy loss: 1.0530254230492215\n",
      "Epoch 300, cross entropy loss: 1.0394197527359592\n",
      "Epoch 400, cross entropy loss: 1.0288989257837184\n",
      "Epoch 500, cross entropy loss: 1.0203698766730955\n",
      "Epoch 600, cross entropy loss: 1.0131823358684602\n",
      "Epoch 700, cross entropy loss: 1.0069353722097847\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.0734983747960911\n",
      "Epoch 200, cross entropy loss: 1.0560871370342944\n",
      "Epoch 300, cross entropy loss: 1.0432446492744694\n",
      "Epoch 400, cross entropy loss: 1.0332270877023189\n",
      "Epoch 500, cross entropy loss: 1.025034590781424\n",
      "Epoch 600, cross entropy loss: 1.018073833799945\n",
      "Epoch 700, cross entropy loss: 1.011979850052689\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.0708460714614316\n",
      "Epoch 200, cross entropy loss: 1.0517373206983984\n",
      "Epoch 300, cross entropy loss: 1.0377746114592992\n",
      "Epoch 400, cross entropy loss: 1.0269953991348002\n",
      "Epoch 500, cross entropy loss: 1.01826847551758\n",
      "Epoch 600, cross entropy loss: 1.0109190110466757\n",
      "Epoch 700, cross entropy loss: 1.0045301351420743\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.073326999182121\n",
      "Epoch 200, cross entropy loss: 1.0560296450563194\n",
      "Epoch 300, cross entropy loss: 1.04345348051585\n",
      "Epoch 400, cross entropy loss: 1.0337798000226774\n",
      "Epoch 500, cross entropy loss: 1.0259659604630693\n",
      "Epoch 600, cross entropy loss: 1.019393971440446\n",
      "Epoch 700, cross entropy loss: 1.0136846333989045\n",
      "Learning rate: 0.001\n",
      "Regularization: 0.05\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9942352713333616\n",
      "Epoch 200, cross entropy loss: 0.9590574463866668\n",
      "Epoch 300, cross entropy loss: 0.9321539672245965\n",
      "Epoch 400, cross entropy loss: 0.9094577794314816\n",
      "Epoch 500, cross entropy loss: 0.890052263546086\n",
      "Epoch 600, cross entropy loss: 0.8735161523060765\n",
      "Epoch 700, cross entropy loss: 0.8595069408302662\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9915812127629855\n",
      "Epoch 200, cross entropy loss: 0.9562619437882329\n",
      "Epoch 300, cross entropy loss: 0.9297316030438585\n",
      "Epoch 400, cross entropy loss: 0.9075917960916107\n",
      "Epoch 500, cross entropy loss: 0.888774106678319\n",
      "Epoch 600, cross entropy loss: 0.8727798984666366\n",
      "Epoch 700, cross entropy loss: 0.8592302836648481\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.996847667584245\n",
      "Epoch 200, cross entropy loss: 0.9615035608421625\n",
      "Epoch 300, cross entropy loss: 0.9348733782344042\n",
      "Epoch 400, cross entropy loss: 0.9127383584210021\n",
      "Epoch 500, cross entropy loss: 0.8940465160410322\n",
      "Epoch 600, cross entropy loss: 0.8782955620904894\n",
      "Epoch 700, cross entropy loss: 0.8650954408004786\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.988785600608393\n",
      "Epoch 200, cross entropy loss: 0.9518830545743627\n",
      "Epoch 300, cross entropy loss: 0.9234466992136959\n",
      "Epoch 400, cross entropy loss: 0.899457173583709\n",
      "Epoch 500, cross entropy loss: 0.8790292999790751\n",
      "Epoch 600, cross entropy loss: 0.8617247323969656\n",
      "Epoch 700, cross entropy loss: 0.8471637367051763\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9996236750802437\n",
      "Epoch 200, cross entropy loss: 0.9665796463634941\n",
      "Epoch 300, cross entropy loss: 0.9407081576207236\n",
      "Epoch 400, cross entropy loss: 0.9183450957860689\n",
      "Epoch 500, cross entropy loss: 0.89881365945899\n",
      "Epoch 600, cross entropy loss: 0.8818597836531412\n",
      "Epoch 700, cross entropy loss: 0.8672586428673166\n",
      "Learning rate: 0.01\n",
      "Regularization: 0.05\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8289493998248025\n",
      "Epoch 200, cross entropy loss: 0.7917088345854054\n",
      "Epoch 300, cross entropy loss: 0.7820714091908616\n",
      "Epoch 400, cross entropy loss: 0.7781482839478043\n",
      "Epoch 500, cross entropy loss: 0.7762943248650012\n",
      "Epoch 600, cross entropy loss: 0.7756219281448524\n",
      "Epoch 700, cross entropy loss: 0.7757785844465405\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8295164619194235\n",
      "Epoch 200, cross entropy loss: 0.7914090269077898\n",
      "Epoch 300, cross entropy loss: 0.7797126487299209\n",
      "Epoch 400, cross entropy loss: 0.7741881885748277\n",
      "Epoch 500, cross entropy loss: 0.7713351956599696\n",
      "Epoch 600, cross entropy loss: 0.7701414867320353\n",
      "Epoch 700, cross entropy loss: 0.7701105556132839\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8369026739348563\n",
      "Epoch 200, cross entropy loss: 0.8058485360888072\n",
      "Epoch 300, cross entropy loss: 0.8005820743061645\n",
      "Epoch 400, cross entropy loss: 0.7998900906938535\n",
      "Epoch 500, cross entropy loss: 0.8004159627195342\n",
      "Epoch 600, cross entropy loss: 0.8014623262034206\n",
      "Epoch 700, cross entropy loss: 0.8028253073574496\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8158241338129407\n",
      "Epoch 200, cross entropy loss: 0.7795553441296085\n",
      "Epoch 300, cross entropy loss: 0.7716655053817559\n",
      "Epoch 400, cross entropy loss: 0.7692686225450251\n",
      "Epoch 500, cross entropy loss: 0.7686484461428698\n",
      "Epoch 600, cross entropy loss: 0.7689625877537279\n",
      "Epoch 700, cross entropy loss: 0.7699109232760173\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8345652842183122\n",
      "Epoch 200, cross entropy loss: 0.7916302830389554\n",
      "Epoch 300, cross entropy loss: 0.7786935085116842\n",
      "Epoch 400, cross entropy loss: 0.7726431741261898\n",
      "Epoch 500, cross entropy loss: 0.7693742203805884\n",
      "Epoch 600, cross entropy loss: 0.7677771626134179\n",
      "Epoch 700, cross entropy loss: 0.7673573492780125\n",
      "Learning rate: 0.1\n",
      "Regularization: 0.05\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.0723782168952205\n",
      "Epoch 200, cross entropy loss: 1.0543838497265121\n",
      "Epoch 300, cross entropy loss: 1.0412723560922146\n",
      "Epoch 400, cross entropy loss: 1.031174271674815\n",
      "Epoch 500, cross entropy loss: 1.023016595549357\n",
      "Epoch 600, cross entropy loss: 1.0161620508308185\n",
      "Epoch 700, cross entropy loss: 1.0102185194631368\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.071662922852887\n",
      "Epoch 200, cross entropy loss: 1.0531734608036099\n",
      "Epoch 300, cross entropy loss: 1.0397017546268439\n",
      "Epoch 400, cross entropy loss: 1.0293313633048946\n",
      "Epoch 500, cross entropy loss: 1.0209621031311265\n",
      "Epoch 600, cross entropy loss: 1.0139400781794228\n",
      "Epoch 700, cross entropy loss: 1.0078624689019728\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.0735403564652775\n",
      "Epoch 200, cross entropy loss: 1.0562268931135217\n",
      "Epoch 300, cross entropy loss: 1.043511971165419\n",
      "Epoch 400, cross entropy loss: 1.0336386024576105\n",
      "Epoch 500, cross entropy loss: 1.0256001933989392\n",
      "Epoch 600, cross entropy loss: 1.0187999097288363\n",
      "Epoch 700, cross entropy loss: 1.0128708882932456\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.0708903574828592\n",
      "Epoch 200, cross entropy loss: 1.0518836746149298\n",
      "Epoch 300, cross entropy loss: 1.0380526317153351\n",
      "Epoch 400, cross entropy loss: 1.0274206659239185\n",
      "Epoch 500, cross entropy loss: 1.0188496065350723\n",
      "Epoch 600, cross entropy loss: 1.011661168340945\n",
      "Epoch 700, cross entropy loss: 1.0054367581080643\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.0733702828105474\n",
      "Epoch 200, cross entropy loss: 1.0561734272931096\n",
      "Epoch 300, cross entropy loss: 1.0437279416497929\n",
      "Epoch 400, cross entropy loss: 1.0342015037269934\n",
      "Epoch 500, cross entropy loss: 1.0265445719219677\n",
      "Epoch 600, cross entropy loss: 1.0201356169559612\n",
      "Epoch 700, cross entropy loss: 1.0145935755161513\n",
      "Learning rate: 0.001\n",
      "Regularization: 0.1\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9956629601045246\n",
      "Epoch 200, cross entropy loss: 0.9622739320715408\n",
      "Epoch 300, cross entropy loss: 0.9371331273461763\n",
      "Epoch 400, cross entropy loss: 0.9161021698423286\n",
      "Epoch 500, cross entropy loss: 0.8982536379524757\n",
      "Epoch 600, cross entropy loss: 0.8831705283337666\n",
      "Epoch 700, cross entropy loss: 0.870517512411385\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9930366262305618\n",
      "Epoch 200, cross entropy loss: 0.9595211442215946\n",
      "Epoch 300, cross entropy loss: 0.9347569507606878\n",
      "Epoch 400, cross entropy loss: 0.9142812427541194\n",
      "Epoch 500, cross entropy loss: 0.8970183593812732\n",
      "Epoch 600, cross entropy loss: 0.8824751272420753\n",
      "Epoch 700, cross entropy loss: 0.8702799647744999\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9982568146688325\n",
      "Epoch 200, cross entropy loss: 0.9646989664719482\n",
      "Epoch 300, cross entropy loss: 0.9398206504834499\n",
      "Epoch 400, cross entropy loss: 0.9193330797790925\n",
      "Epoch 500, cross entropy loss: 0.9021771546416463\n",
      "Epoch 600, cross entropy loss: 0.887856915640657\n",
      "Epoch 700, cross entropy loss: 0.8759903525441062\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.9902047002021176\n",
      "Epoch 200, cross entropy loss: 0.9550663183787087\n",
      "Epoch 300, cross entropy loss: 0.9283883220100208\n",
      "Epoch 400, cross entropy loss: 0.9060769703556383\n",
      "Epoch 500, cross entropy loss: 0.887229638731892\n",
      "Epoch 600, cross entropy loss: 0.8714077590758762\n",
      "Epoch 700, cross entropy loss: 0.8582360501045782\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.0010570357198088\n",
      "Epoch 200, cross entropy loss: 0.9698231412033037\n",
      "Epoch 300, cross entropy loss: 0.9457366303348512\n",
      "Epoch 400, cross entropy loss: 0.9250561369142033\n",
      "Epoch 500, cross entropy loss: 0.907092868312742\n",
      "Epoch 600, cross entropy loss: 0.8915979760546562\n",
      "Epoch 700, cross entropy loss: 0.8783554722190416\n",
      "Learning rate: 0.01\n",
      "Regularization: 0.1\n",
      "nan\n",
      "[    0  7498 14995 22492 29989]\n",
      "[    0  7498 14995 22492 29989]\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8435932112698702\n",
      "Epoch 200, cross entropy loss: 0.8147859746125149\n",
      "Epoch 300, cross entropy loss: 0.8113542270851993\n",
      "Epoch 400, cross entropy loss: 0.8129507318044146\n",
      "Epoch 500, cross entropy loss: 0.8163457535629929\n",
      "Epoch 600, cross entropy loss: 0.8207779968080936\n",
      "Epoch 700, cross entropy loss: 0.8259485794033503\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.844193359947907\n",
      "Epoch 200, cross entropy loss: 0.814492020331691\n",
      "Epoch 300, cross entropy loss: 0.8089986656104212\n",
      "Epoch 400, cross entropy loss: 0.8090185532464018\n",
      "Epoch 500, cross entropy loss: 0.8114522296298827\n",
      "Epoch 600, cross entropy loss: 0.8154022596387228\n",
      "Epoch 700, cross entropy loss: 0.8204183785666745\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8513615665808953\n",
      "Epoch 200, cross entropy loss: 0.8284874302461999\n",
      "Epoch 300, cross entropy loss: 0.8291503692201299\n",
      "Epoch 400, cross entropy loss: 0.8336982222487759\n",
      "Epoch 500, cross entropy loss: 0.8391917128501885\n",
      "Epoch 600, cross entropy loss: 0.8450617555935941\n",
      "Epoch 700, cross entropy loss: 0.8511621994193621\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8306374788167359\n",
      "Epoch 200, cross entropy loss: 0.803093705775259\n",
      "Epoch 300, cross entropy loss: 0.8015783769573801\n",
      "Epoch 400, cross entropy loss: 0.8048045140069015\n",
      "Epoch 500, cross entropy loss: 0.8095052548212184\n",
      "Epoch 600, cross entropy loss: 0.8149771921916087\n",
      "Epoch 700, cross entropy loss: 0.8209785827981021\n",
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 0.8492961439880723\n",
      "Epoch 200, cross entropy loss: 0.8149028112963149\n",
      "Epoch 300, cross entropy loss: 0.8084257661501423\n",
      "Epoch 400, cross entropy loss: 0.8082084095648092\n",
      "Epoch 500, cross entropy loss: 0.8105300101816826\n",
      "Epoch 600, cross entropy loss: 0.8143971545924236\n",
      "Epoch 700, cross entropy loss: 0.8193640701862465\n",
      "Learning rate: 0.1\n",
      "Regularization: 0.1\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "regularizers = np.array([0.01, 0.05, 0.1])\n",
    "learning_rates = np.array([0.001, 0.01, 0.1])\n",
    "\n",
    "params_combination = np.array(\n",
    "    np.meshgrid(regularizers, learning_rates)\n",
    ").T.reshape(-1, 2)\n",
    "\n",
    "cross_validation_accuracies = []\n",
    "cross_validation_accuracies_means = []\n",
    "train_errors = []\n",
    "validation_errors = []\n",
    "\n",
    "for reg, l_rate in params_combination:\n",
    "    model = MultinomialLogisticRegression(learning_rate=l_rate, num_iterations=800, regularizer=reg)\n",
    "    model.cross_validation(X_train, y_train)\n",
    "    print(\"Learning rate:\", l_rate)\n",
    "    print(\"Regularization:\", reg)\n",
    "    cross_validation_accuracies.append(model.cross_validation_accuracy)\n",
    "    cross_validation_accuracies_means.append(np.mean(model.cross_validation_accuracy))\n",
    "    train_errors.append(model.cross_valid_train_errors)\n",
    "    validation_errors.append(model.cross_valid_valid_errors)\n",
    "    \n",
    "    print(np.mean(model.cross_validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "print(np.min(cross_validation_accuracies))\n",
    "opt_params = params_combination[np.argmin(cross_validation_accuracies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8091299469907995"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.mean(np.array(cross_validation_accuracies)[:,1:], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.mean(np.array(cross_validation_accuracies)[:,1:], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.1 ])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_combination[np.argmax(np.mean(np.array(cross_validation_accuracies)[:,1:], axis=1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, cross entropy loss: 1.0986122886681093\n",
      "Epoch 100, cross entropy loss: 0.7491199756241687\n",
      "Epoch 200, cross entropy loss: 0.7270739643569134\n",
      "Epoch 300, cross entropy loss: 0.7189967208008884\n",
      "Epoch 400, cross entropy loss: 0.7145454106615231\n",
      "Epoch 500, cross entropy loss: 0.7120580900130821\n",
      "Epoch 600, cross entropy loss: 0.7107566065165323\n",
      "Epoch 700, cross entropy loss: 0.71020573480836\n",
      "Epoch 800, cross entropy loss: 0.710172058542407\n",
      "Epoch 900, cross entropy loss: 0.7105269701398971\n",
      "Index(['lat', 'TS', 'Z1000', 'U850', 'PS', 'PSL', 'TMQ', 'Z200', 'lon', 'T500',\n",
      "       'Label'],\n",
      "      dtype='object')\n",
      "Confusion Matrix:\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [1 0 0]] \n",
      "\n",
      "Accuracy:\n",
      "0.7859 \n",
      "\n",
      "Precision:\n",
      "0.262 \n",
      "\n",
      "Recall:\n",
      "0.3333 \n",
      "\n",
      "F1 Score:\n",
      "0.2934\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialLogisticRegression(learning_rate=0.1, num_iterations=1000, regularizer=0.05)\n",
    "model.fit(X_train, y_train, collist=raw_data.columns, valid_x=X_train, valid_y=y_train)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.predict(X_valid)\n",
    "print(train_df.columns)\n",
    "\n",
    "model.get_metrics(y_train, predictions, return_values=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x174121ca0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(\n",
    "    model.iters,\n",
    "    model.train_errors\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    model.iters,\n",
    "    model.valid_errors,\n",
    "    color='r'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predictions_2023-10-25 23:37:22.889423_opt_multinomial_all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['T200', 'Z200', 'VBOT', 'UBOT', 'T500', 'U850', 'TS', 'TMQ', 'V850',\n",
      "       'Label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "preproc = Preprocess()\n",
    "raw_data = preproc.load_data(raw_train_data)\n",
    "\n",
    "cols = [\"T200\", \"Z200\", \"VBOT\", \"UBOT\",  \"T500\", \"U850\", \"TS\", \"TMQ\", \"V850\",\n",
    "        \"Label\"] \n",
    "\n",
    "train_df, train_data = preproc.preprocess_data(raw_data, drop_cols=[\"SNo\", \"time\"])\n",
    "train_df = train_df[cols]\n",
    "\n",
    "np.random.shuffle(train_data)\n",
    "X_train, y_train, X_valid, y_valid = preproc.train_valid_split(\n",
    "    train_df.to_numpy(), test_size=0.33 #, random_state=42\n",
    ")\n",
    "\n",
    "print(train_df.columns)\n",
    "\n",
    "X_train = preproc.normalize_data(X_train)\n",
    "X_valid = preproc.normalize_data(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, cross entropy loss: 1.0986122886681096\n",
      "Epoch 100, cross entropy loss: 1.3319577682487105\n",
      "Epoch 200, cross entropy loss: 1.3522162466581074\n",
      "Epoch 300, cross entropy loss: 1.3440761903632488\n",
      "Epoch 400, cross entropy loss: 1.3380537770247012\n",
      "Epoch 500, cross entropy loss: 1.336133265179013\n",
      "Epoch 600, cross entropy loss: 1.3364211734399578\n",
      "Epoch 700, cross entropy loss: 1.3375626636866498\n",
      "Epoch 800, cross entropy loss: 1.3389168156016933\n",
      "Epoch 900, cross entropy loss: 1.3402252040101712\n",
      "Index(['T200', 'Z200', 'VBOT', 'UBOT', 'T500', 'U850', 'TS', 'TMQ', 'V850',\n",
      "       'Label'],\n",
      "      dtype='object')\n",
      "Confusion Matrix:\n",
      "[[3534 5259 2836]\n",
      " [  48  482   48]\n",
      " [ 226  667 1671]] \n",
      "\n",
      "Accuracy:\n",
      "0.385 \n",
      "\n",
      "Precision:\n",
      "0.4567 \n",
      "\n",
      "Recall:\n",
      "0.5965 \n",
      "\n",
      "F1 Score:\n",
      "0.5173\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialLogisticRegression(learning_rate=0.1, num_iterations=1000, regularizer=0.05)\n",
    "model.fit(X_train, y_train, collist=raw_data.columns, valid_x=X_valid, valid_y=y_valid)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.predict(X_valid)\n",
    "print(train_df.columns)\n",
    "\n",
    "model.get_metrics(y_valid, predictions, return_values=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = Preprocess()\n",
    "raw_data = preproc.load_data(raw_test_data)\n",
    "\n",
    "test_df, test_data = preproc.preprocess_data(raw_data, drop_cols=[\"SNo\", \"time\"], is_test=True)\n",
    "\n",
    "test_data = test_df[['lat', 'TS', 'Z1000', 'U850', 'PS', 'PSL', 'TMQ', 'Z200', 'lon', 'T500']].to_numpy()\n",
    "\n",
    "test_data = preproc.normalize_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10320, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(X=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "submition = raw_data['SNo'].reset_index().copy()\n",
    "submition['Label'] = pd.Series(y_pred_test)\n",
    "submition.drop(\"index\", axis=1, inplace=True)\n",
    "submition.to_csv(f\"predictions_{datetime.now()}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dc1-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
